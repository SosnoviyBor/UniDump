### Всім, так би мовити, ✌️.

Для мене на цьому курсі, якщо чесно, стало відкриттям, наскільки нейронні мережі насправді прості. Система взаємопов'язаних лінійних рівнянь - та й лише.

<img src="static/neural network.png" width=500>

Ідея нейронної мережі заключається в чому - будь-який вузол передає зважені значення усім вузлам наступного рівня. Завдяки цьому досягається більш, скажімо, "свідомий" та точно зважений результат у порівнянні з умовною лінійною регресією.

Добре, а як же тоді відбувається тренування подібних моделей? Ось тут і виходить на сцену зворотнє поширення. Якщо не поглиблюватись в математику, то маємо наступну картину: пропустивши вхідні дані через нейронну мережу ми порівнюємо отриманий результат з реальними значеннями та на основі цього калібруємо ваги зв'язків між вихідними вузлами та попереднім шаром нейронів, щоб вони більше відповідали реальності. А потім на основі цього робимо те саме із попереднім набором зв'язків - і так каскадом аж до самого шару вхідних даних. Це й є зворотнє поширення / backward propagation / backpropagation.

Цей підхід безумовно ефективний для навчання (як мінімум, на даний момент) але, тим не менш, достатньо прожерливий по ресурсам, якщо мова йде про великі мережі. Тому зазвичай зворотнє поширення викликають не на кожній ітерації тренування моделі, а з деяким інтервалом, даючи йому усереднені результати контрольних даних.

### Посилання на додаткову літературу

1. [Онлайн-ресурс](https://www.3blue1brown.com/topics/neural-networks) "Neural Networks" Grant Sanderson